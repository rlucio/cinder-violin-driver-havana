# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 Violin Memory, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Violin Memory iSCSI Driver for Openstack Cinder

Uses Violin REST API via XG-Tools to manage a standard V6000 series
flash array to provide network block-storage services.

by Ryan Lucio
Senior Software Engineer
Violin Memory

---

Driver support (verified for G5.5.2):
-------------------------------------
Driver Setup:                   YES
Volume Create/Delete:           YES
Export Create/Remove:           YES
Volume Attach/Detach:           YES
Snapshot Create/Delete:         NO
Create Volume from Snapshot:    NO
Clone Volume:                   YES
Get Volume Stats:               YES
Copy Image to Volume:           YES*
Copy Volume to Image:           YES*

* functionality inherited from base class driver
"""

import random
import time

from oslo.config import cfg

from cinder import context
from cinder import exception
from cinder.openstack.common import log as logging
from cinder.openstack.common import timeutils
from cinder.volume.driver import ISCSIDriver
from cinder.volume import volume_types

LOG = logging.getLogger(__name__)

try:
    from . import version
    __version__ = version.__version__
except Exception:
    # version.py is autogenerated during packaging. If we are running
    # against original source it will not be present.
    __version__ = "unknown"

try:
    from . import vxg
    from .vxg.core.node import XGNode
    from .vxg.core.session import XGSession
except ImportError:
    LOG.exception(
        _("The Violin v6000 driver for Cinder requires the presence of "
          "the Violin 'XG-Tools', python libraries for facilitating "
          "communication between applications and the v6000 XML API. "
          "The libraries can be downloaded from the Violin Memory "
          "support website at http://www.violin-memory.com/support"))
    raise
else:
    LOG.info(_("Running with xg-tools version: %s") % vxg.__version__)

violin_opts = [
    cfg.StrOpt('gateway_vip',
               default='',
               help='IP address or hostname of the v6000 master VIP'),
    cfg.StrOpt('gateway_mga',
               default='',
               help='IP address or hostname of mg-a'),
    cfg.StrOpt('gateway_mgb',
               default='',
               help='IP address or hostname of mg-b'),
    cfg.StrOpt('gateway_user',
               default='admin',
               help='User name for connecting to the Memory Gateway'),
    cfg.StrOpt('gateway_password',
               default='',
               help='User name for connecting to the Memory Gateway',
               secret=True),
    cfg.IntOpt('gateway_iscsi_port',
               default=3260,
               help='IP port to use for iSCSI targets'),
    cfg.StrOpt('gateway_iscsi_target_prefix',
               default='iqn.2004-02.com.vmem:',
               help='prefix for iscsi volumes'),
    cfg.BoolOpt('use_igroups',
                default=False,
                help='Use igroups to manage targets and initiators'),
    cfg.BoolOpt('use_thin_luns',
                default=False,
                help='Use thin luns instead of thick luns'), ]

CONF = cfg.CONF
CONF.register_opts(violin_opts)


class InvalidBackendConfig(exception.CinderException):
    message = _("Volume backend config is invalid: %(reason)s")


class RequestRetryTimeout(exception.CinderException):
    message = _("Backend service retry timeout hit: %(timeout)s sec")


class ViolinBackendErr(exception.CinderException):
    message = _("Backend reports: %(message)s")


class ViolinBackendErrExists(exception.CinderException):
    message = _("Backend reports: item already exists")


class ViolinBackendErrNotFound(exception.CinderException):
    message = _("Backend reports: item not found")


class ViolinDriver(ISCSIDriver):
    """Executes commands relating to Violin Memory Arrays """

    def __init__(self, *args, **kwargs):
        super(ViolinDriver, self).__init__(*args, **kwargs)
        self.session_start_time = 0
        self.session_timeout = 900
        self.request_timeout = 300
        self.array_info = []
        self.vmem_vip = None
        self.vmem_mga = None
        self.vmem_mgb = None
        self.container = ""
        self.stats = {}
        self.gateway_iscsi_ip_addresses_mga = []
        self.gateway_iscsi_ip_addresses_mgb = []
        self.config = kwargs.get('configuration', None)
        self.context = None
        if self.config:
            self.config.append_config_values(violin_opts)

        LOG.info(_("Initialized driver %(name)s version: %(vers)s") %
                 {'name': self.__class__.__name__, 'vers': __version__})

    def do_setup(self, context):
        """Any initialization the driver does while starting """
        if not self.config.gateway_vip:
            raise exception.InvalidInput(
                reason=_('Gateway VIP is not set'))
        if not self.config.gateway_mga:
            raise exception.InvalidInput(
                reason=_('Gateway IP for mg-a is not set'))
        if not self.config.gateway_mgb:
            raise exception.InvalidInput(
                reason=_('Gateway IP for mg-b is not set'))

        self.vmem_vip = vxg.open(self.config.gateway_vip,
                                 self.config.gateway_user,
                                 self.config.gateway_password)
        self.vmem_mga = vxg.open(self.config.gateway_mga,
                                 self.config.gateway_user,
                                 self.config.gateway_password)
        self.vmem_mgb = vxg.open(self.config.gateway_mgb,
                                 self.config.gateway_user,
                                 self.config.gateway_password)
        self.context = context

        self.gateway_iscsi_ip_addresses_mga = self._get_active_iscsi_ips(
            self.vmem_mga)
        for ip in self.gateway_iscsi_ip_addresses_mga:
            self.array_info.append({"node": self._get_hostname('mga'),
                                    "addr": ip,
                                    "conn": self.vmem_mga})
        self.gateway_iscsi_ip_addresses_mgb = self._get_active_iscsi_ips(
            self.vmem_mgb)
        for ip in self.gateway_iscsi_ip_addresses_mgb:
            self.array_info.append({"node": self._get_hostname('mgb'),
                                    "addr": ip,
                                    "conn": self.vmem_mgb})

        vip = self.vmem_vip.basic

        ret_dict = vip.get_node_values("/vshare/state/local/container/*")
        if ret_dict:
            self.container = ret_dict.items()[0][1]
        ret_dict = vip.get_node_values("/wsm/inactivity_timeout")
        if ret_dict:
            self.session_timeout = ret_dict.items()[0][1]

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met"""
        vip = self.vmem_vip.basic

        if len(self.container) == 0:
            raise InvalidBackendConfig(reason=_('container is missing'))

        bn = "/vshare/config/iscsi/enable"
        resp = vip.get_node_values(bn)
        if resp[bn] != True:
            raise InvalidBackendConfig(reason=_('iSCSI is not enabled'))

        if len(self.gateway_iscsi_ip_addresses_mga) == 0:
            raise InvalidBackendConfig(reason=
                                       _('no available iSCSI IPs on mga'))
        if len(self.gateway_iscsi_ip_addresses_mgb) == 0:
            raise InvalidBackendConfig(reason=
                                       _('no available iSCSI IPs on mgb'))

    def create_volume(self, volume):
        """Creates a volume """
        self._login()
        self._create_lun(volume)

    def delete_volume(self, volume):
        """Deletes a volume """
        self._login()
        self._delete_lun(volume)

    def create_volume_from_snapshot(self, volume, snapshot):
        """Creates a volume from a snapshot """
        # NYI (RDL: The V6000's 5.x.x platform does not support Data
        # Management features)
        #
        raise NotImplementedError

    def create_cloned_volume(self, volume, src_vref):
        """Creates a clone of the specified volume."""
        self._login()
        self._create_lun(volume)
        self.copy_volume_data(self.context, src_vref, volume)

    def create_snapshot(self, snapshot):
        """Creates a snapshot from an existing volume """
        # NYI (RDL: The V6000's 5.x.x platform does not support Data
        # Management features)
        #
        raise NotImplementedError

    def delete_snapshot(self, snapshot):
        """Deletes a snapshot """
        # NYI (RDL: The V6000's 5.x.x platform does not support Data
        # Management features)
        #
        raise NotImplementedError

    def ensure_export(self, context, volume):
        """Synchronously checks and re-exports volumes at cinder start time """
        pass

    def create_export(self, context, volume):
        """Exports the volume """
        pass

    def remove_export(self, context, volume):
        """Removes an export for a logical volume """
        pass

    def initialize_connection(self, volume, connector):
        """Initializes the connection (target<-->initiator) """
        self._login()

        igroup = None

        if self.config.use_igroups:
            #
            # Most drivers don't use igroups, because there are a
            # number of issues with multipathing and iscsi/fcp where
            # lun devices either aren't cleaned up properly or are
            # stale (from previous scans).
            #

            # If the customer really wants igroups for whatever
            # reason, we create a new igroup for each host/hypervisor.
            # Every lun that is exported to the particular
            # hypervisor/host will be contained in this igroup.  This
            # should prevent other hosts from seeing luns they aren't
            # using when they perform scans.
            #
            igroup = self._get_igroup(volume, connector)
            self._add_igroup_member(connector, igroup)

        vol = self._get_short_name(volume['name'])
        tgt = self._create_iscsi_target(volume)
        lun = self._export_lun(volume, connector, igroup)
        iqn = "%s%s:%s" % (self.config.gateway_iscsi_target_prefix,
                           tgt['node'], vol)
        self.vmem_vip.basic.save_config()

        properties = {}
        properties['target_discovered'] = False
        properties['target_portal'] = '%s:%s' % (tgt['addr'], '3260')
        properties['target_iqn'] = iqn
        properties['target_lun'] = lun
        properties['volume_id'] = volume['id']
        properties['auth_method'] = 'CHAP'
        properties['auth_username'] = ''
        properties['auth_password'] = ''

        return {'driver_volume_type': 'iscsi', 'data': properties}

    def terminate_connection(self, volume, connector, force=False, **kwargs):
        """Terminates the connection (target<-->initiator) """
        self._login()
        self._unexport_lun(volume)
        self._delete_iscsi_target(volume)
        self.vmem_vip.basic.save_config()

    def get_volume_stats(self, refresh=False):
        """Get volume stats """
        if refresh or not self.stats:
            self._login()
            self._update_stats()
        return self.stats

    def _create_lun(self, volume):
        """
        Creates a new lun.

        The equivalent CLI command is "lun create container
        <container_name> name <lun_name> size <gb>"

        Arguments:
            volume -- volume object provided by the Manager
        """
        lun_type = '0'
        v = self.vmem_vip

        LOG.info(_("Creating lun %(name)s, %(size)s GB") % volume)

        # TODO(rdl): thin luns are NOT supported in 5.x, so this
        # will not work (not sure if backend will ignore the flag)
        #
        if self.config.use_thin_luns:
            lun_type = '1'

        # using the defaults for other fields: (quantity, nozero,
        # readonly, startnum, blksize)
        #
        try:
            self._send_cmd(v.lun.create_lun,
                           'LUN create: success!',
                           self.container, volume['name'],
                           volume['size'], 1, '0', lun_type, 'w', 1, 512)

        except ViolinBackendErrExists:
            LOG.info(_("Lun %s already exists, continuing"), volume['name'])

        except Exception:
            LOG.info(_("Lun create failed!"))
            raise

    def _delete_lun(self, volume):
        """
        Deletes a lun.

        The equivalent CLI command is "no lun create container
        <container_name> name <lun_name>"

        Arguments:
            volume -- volume object provided by the Manager
        """
        v = self.vmem_vip

        LOG.info(_("Deleting lun %s"), volume['name'])

        try:
            self._send_cmd(v.lun.bulk_delete_luns,
                           'LUN deletion started',
                           self.container, volume['name'])

        except ViolinBackendErrNotFound:
            LOG.info(_("Lun %s already deleted, continuing"), volume['name'])

        # TODO(RDL): not used in g5.x, backend returns this error
        # stupidly whenever an attempt to grab the lvm lock happens
        # when it is arleady locked.  If this happens during a
        # deletion request, the infrastructure above will give up and
        # mark the lun as available.
        #
        except ViolinBackendErrExists:
            LOG.info(_("Lun %s has dependent snapshots, skipping"),
                     volume['name'])
            raise exception.VolumeIsBusy(volume_name=volume['name'])

        except Exception:
            LOG.exception(_("Lun delete failed!"))
            raise

    def _create_iscsi_target(self, volume):
        """
        Creates a new target for use in exporting a lun

        Openstack does not yet support multipathing. We still create
        HA targets but we pick a single random target for the
        Openstack infrastructure to use.  This at least allows us to
        evenly distribute LUN connections across the storage cluster.
        The equivalent CLI commands are "iscsi target create
        <target_name>" and "iscsi target bind <target_name> to
        <ip_of_mg_eth_intf>".

        Arguments:
            volume -- volume object provided by the Manager

        Returns:
            reference to randomly selected target object
        """
        v = self.vmem_vip
        target_name = self._get_short_name(volume['name'])

        LOG.info(_("Creating iscsi target %s"), target_name)

        try:
            self._send_cmd(v.iscsi.create_iscsi_target,
                           '', target_name)
        except Exception:
            LOG.exception(_("Failed to create iscsi target!"))
            raise

        try:
            self._send_cmd(self.vmem_mga.iscsi.bind_ip_to_target,
                           '', target_name, self.gateway_iscsi_ip_addresses_mga)
            self._send_cmd(self.vmem_mgb.iscsi.bind_ip_to_target,
                           '', target_name, self.gateway_iscsi_ip_addresses_mgb)
        except Exception:
            LOG.exception(_("Failed to bind iSCSI targets!"))
            raise

        return self.array_info[random.randint(0, len(self.array_info) - 1)]

    def _delete_iscsi_target(self, volume):
        """
        Deletes the iscsi target for a lun

        iSCSI targets must be deleted from each gateway separately.
        The CLI equivalent is "no iscsi target create <target_name>".

        Arguments:
            volume -- volume object provided by the Manager
        """
        v = self.vmem_vip
        target_name = self._get_short_name(volume['name'])

        # TODO(rlucio): afterglow+ does not require the user to manually
        # delete both iscsi target bindings before deleting the target
        #

        LOG.info(_("Deleting iscsi target for %s"), target_name)

        try:
            self._send_cmd(self.vmem_mga.iscsi.unbind_ip_from_target,
                           '', target_name, self.gateway_iscsi_ip_addresses_mga)
            self._send_cmd(self.vmem_mgb.iscsi.unbind_ip_from_target,
                           '', target_name, self.gateway_iscsi_ip_addresses_mgb)
        except Exception:
            LOG.exception(_("Failed to unbind iSCSI targets!"))
            raise

        try:
            self._send_cmd(v.iscsi.delete_iscsi_target,
                           '', target_name)
        except Exception:
            LOG.exception(_("Failed to delete iSCSI target!"))
            raise

    def _export_lun(self, volume, connector=None, igroup=None):
        """
        Generates the export configuration for the given volume

        The equivalent CLI command is "lun export container
        <container_name> name <lun_name>"

        Arguments:
            volume -- volume object provided by the Manager
            connector -- connector object provided by the Manager
            igroup -- name of igroup to use for exporting

        Returns:
            lun_id -- the LUN ID assigned by the backend
        """
        lun_id = -1
        export_to = ''
        v = self.vmem_vip

        if igroup:
            export_to = igroup
        elif connector:
            export_to = connector['initiator']
        else:
            raise exception.Error(_("No initiators found, cannot proceed"))

        target_name = self._get_short_name(volume['name'])

        # TODO(rlucio): new encryption code in afterglow requires that
        # lun state nodes for encryption exist before running the
        # export or else the export will fail on the backend (via
        # /vshare/state/local/container/%s/lun/%s/encrypted)
        #

        LOG.info(_("Exporting lun %s"), volume['name'])

        try:
            self._send_cmd(v.lun.export_lun, '',
                           self.container, volume['name'], target_name,
                           export_to, 'auto')

        except Exception:
            LOG.exception(_("LUN export failed!"))
            raise

        else:
            self._wait_for_exportstate(volume['name'], True)
            lun_id = self._get_lun_id(volume['name'])

        return lun_id

    def _unexport_lun(self, volume):
        """
        Removes the export configuration for the given volume.

        The equivalent CLI command is "no lun export container
        <container_name> name <lun_name>"

        Arguments:
            volume -- volume object provided by the Manager
        """
        v = self.vmem_vip

        LOG.info(_("Unexporting lun %s"), volume['name'])

        try:
            self._send_cmd(v.lun.unexport_lun, '',
                           self.container, volume['name'],
                           'all', 'all', 'auto')

        except Exception:
            LOG.exception(_("LUN unexport failed!"))
            raise

        else:
            self._wait_for_exportstate(volume['name'], False)

    def _add_igroup_member(self, connector, igroup):
        """
        Add an initiator to the openstack igroup so it can see exports.

        The equivalent CLI command is "igroup addto name <igroup_name>
        initiators <initiator_name>"

        Arguments:
            connector -- connector object provided by the Manager
        """
        v = self.vmem_vip

        LOG.info(_("Adding initiator %s to igroup"), connector['initiator'])

        resp = v.igroup.add_initiators(igroup, connector['initiator'])

        if resp['code'] != 0:
            raise exception.Error(
                _('Failed to add igroup member: %(code)d, %(message)s') % resp)

    def _update_stats(self):
        """
        Gathers array stats from the backend and converts them to GB values.
        """
        data = {}
        total_gb = 'unknown'
        alloc_gb = 'unknown'
        free_gb = 'unknown'
        v = self.vmem_vip

        # 5.x.x arrays do not have a free_bytes node, so we have to
        # compute it manually.
        #
        bn1 = "/vshare/state/global/1/container/%s/total_bytes" \
            % self.container
        bn2 = "/vshare/state/global/1/container/%s/alloc_bytes" \
            % self.container
        resp = v.basic.get_node_values([bn1, bn2])
        if len(resp.keys()) == 2:
            total_gb = resp[bn1] / 1024 / 1024 / 1024
            alloc_gb = resp[bn2] / 1024 / 1024 / 1024
            free_gb = total_gb - alloc_gb

        backend_name = self.config.volume_backend_name
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['vendor_name'] = 'Violin Memory, Inc.'
        data['driver_version'] = __version__
        data['storage_protocol'] = 'iSCSI'
        data['reserved_percentage'] = 0
        data['QoS_support'] = False
        data['total_capacity_gb'] = total_gb
        data['free_capacity_gb'] = free_gb

        for i in data:
            LOG.debug(_("stat update: %(name)s=%(data)s") %
                      {'name': i, 'data': data[i]})

        self.stats = data

    def _get_short_name(self, volume_name):
        """
        Creates a vSHARE-compatible iSCSI target name.

        The Folsom-style volume names are prefix(7) + uuid(36), which
        is too long for vSHARE for target names.  To keep things
        simple we can just truncate the name to 32 chars.

        Arguments:
            volume_name -- name of volume/lun

        Returns:
            Shortened volume name as a string.
        """
        return volume_name[:32]

    def _get_active_iscsi_ips(self, mg_conn):
        """
        Get a list of gateway IP addresses that can be used for iSCSI.

        Arguments:
            mg_conn -- active XG connection to one of the gateways

        Returns:
            active_gw_iscsi_ips -- list of IP addresses
        """
        active_gw_iscsi_ips = []
        interfaces_to_skip = ['lo', 'vlan10', 'eth1', 'eth2', 'eth3']

        bn = "/net/interface/config/*"
        intf_list = mg_conn.basic.get_node_values(bn)

        for i in intf_list:
            do_skip = False

            for s in interfaces_to_skip:
                if intf_list[i] == s:
                    do_skip = True
                    break

            if not do_skip:
                bn1 = "/net/interface/state/%s/addr/ipv4/1/ip" % intf_list[i]
                bn2 = "/net/interface/state/%s/flags/link_up" % intf_list[i]
                resp = mg_conn.basic.get_node_values([bn1, bn2])

                if len(resp.keys()) == 2 and resp[bn2] == True:
                    active_gw_iscsi_ips.append(resp[bn1])

        return active_gw_iscsi_ips

    def _get_hostname(self, mg_to_query):
        """
        Get the hostname of one of the mgs (hostname is used in IQN).
        If the remote query fails then fall back to using the hostname
        provided in the cinder configuration file.

        Arguments:
            mg_to_query -- name of gateway to query 'mga' or 'mgb'

        Returns: hostname -- hostname as a string
        """
        hostname = self.config.gateway_vip
        conn = self.vmem_vip.basic

        if mg_to_query == "mga":
            hostname = self.config.gateway_mga
            conn = self.vmem_mga.basic
        elif mg_to_query == "mgb":
            hostname = self.config.gateway_mgb
            conn = self.vmem_mgb.basic

        ret_dict = conn.get_node_values("/system/hostname")
        if ret_dict:
            hostname = ret_dict.items()[0][1]
        else:
            LOG.debug(_("Unable to fetch gateway hostname for %s"),
                      mg_to_query)

        return hostname

##################################################################################
##################################################################################
##################################################################################

    def _login(self, force=False):
        """
        Get new api creds from the backend, only if needed.

        Arguments:
            force -- re-login on all sessions regardless of last login time

        Returns:
           True if sessions were refreshed, false otherwise.
        """
        now = time.time()
        if abs(now - self.session_start_time) >= self.session_timeout or \
                force == True:
            self.vmem_vip.basic.login()
            self.vmem_mga.basic.login()
            self.vmem_mgb.basic.login()
            self.session_start_time = now
            return True
        return False

    def _send_cmd(self, request_func, success_msg, *args):
        """
        Run an XG request function, and retry every 0-5 seconds until the
        request returns a success message, a failure message, or the global
        request timeout is hit.

        This wrapper is meant to deal with backend requests that can
        fail for any variety of reasons, for instance, when the system
        is already busy handling other LUN requests.  It is also smart
        enough to give up if clustering is down (eg no HA available),
        there is no space left, or other "fatal" errors are returned
        (see _fatal_error_code() for a list of all known error
        conditions).

        Arguments:
            request_func    -- XG api method to call
            success_msg     -- Success message expected from the backend
            *args           -- argument array to be passed to the request_func

        Returns:
            The response dict from the last XG call.
        """
        resp = {}
        start = time.time()

        while True:
            if time.time() - start >= self.request_timeout:
                raise RequestRetryTimeout(timeout=self.request_timeout)

            time.sleep(random.randint(0, 5))
            resp = request_func(*args)

            if not resp['message']:
                # XG requests will return None for a message if no message
                # string is passed int the raw response
                resp['message'] = ''

            if not resp['code'] and success_msg in resp['message']:
                break

            self._fatal_error_code(resp)

        return resp

    def _get_lun_id(self, volume_name):
        """
        Queries the gateway to find the lun id for the exported
        volume.  Technically a lun id is assigned for each target, but
        it is the same value for all targets.

        Arguments:
            volume_name    -- LUN to query

        Returns:
            LUN ID for the exported lun as an integer.  If no LUN ID
            is found, return -1.
        """
        vip = self.vmem_vip.basic
        lun_id = -1

        prefix = "/vshare/config/export/container"
        bn = "%s/%s/lun/%s/target/**" \
            % (prefix, self.container, volume_name)
        resp = vip.get_node_values(bn)

        # EX: /vshare/config/export/container/PROD08/lun/test1/target/hba-b2/
        #     initiator/openstack/lun_id = 1 (int16)
        #
        for node in resp:
            if node.endswith('/lun_id'):
                lun_id = resp[node]
                break

        # TODO(rdl): add exception for case where no lun id found, or lun ids
        # do not match
        #
        return lun_id

    def _get_igroup(self, volume, connector):
        """
        Gets the igroup that should be used when configuring a volume

        Arguments:
            volume: volume object used to determine the igroup name

        Returns:
            igroup_name: name of igroup (for configuring targets & initiators)
        """
        v = self.vmem_vip

        # Use the connector's primary hostname and use that as the
        # name of the igroup.  The name must follow syntax rules
        # required by the array: "must contain only alphanumeric
        # characters, dashes, and underscores.  The first character
        # must be alphanumeric".
        #
        igroup_name = re.sub(r'[\W]', '_',connector['host'])

        # verify that the igroup has been created on the backend, and
        # if it doesn't exist, create it!
        #
        bn = "/vshare/config/igroup/%s" % igroup_name
        resp = v.basic.get_node_values(bn)

        if not len(resp):
            v.igroup.create_igroup(igroup_name)

        return igroup_name

    def _get_volume_type_extra_spec(self, volume, spec_key):
        """
        Code adapted from examples in
        cinder/volume/drivers/solidfire.py and
        cinder/openstack/common/scheduler/filters/capabilities_filter.py.
        """
        spec_value = None
        ctxt = context.get_admin_context()
        typeid = volume['volume_type_id']
        if typeid:
            volume_type = volume_types.get_volume_type(ctxt, typeid)
            volume_specs = volume_type.get('extra_specs')
            for key, val in volume_specs.iteritems():

                # Havana release altered extra_specs to require a
                # prefix on all non-host-capability related extra
                # specs, so that prefix is dropped here before
                # checking the key.
                #
                if ':' in key:
                    scope = key.split(':')

                if scope[1] == spec_key:
                    spec_value = val
                    break

        return spec_value

    def _wait_for_exportstate(self, volume_name, state=False):
        """
        Polls volume's export configuration root.

        XG sets/queries following a request to create or delete a
        lun export may fail on the backend if vshared is still
        processing the export action.  We can check whether it is
        done by polling the export binding for a lun to
        ensure it is created or deleted.

        Arguments:
            volume_name -- name of volume to be polled
            state       -- True to poll for existence, False for lack of

        Returns:
            True if the export state was eventually found, false otherwise.
        """
        status = False
        vip = self.vmem_vip.basic

        # TODO(rdl): this implementation only waits on the master, but
        # may need to additionally wait for the standby to finish the
        # config sync
        #

        bn = "/vshare/config/export/container/%s/lun/%s" \
            % (self.container, volume_name)

        for i in xrange(30):
            resp = vip.get_node_values(bn)
            if state and len(resp.keys()):
                status = True
                break
            elif (not state) and (not len(resp.keys())):
                break
            else:
                time.sleep(1)
        return status

    def _fatal_error_code(self, response):
        """
        Check the error code in a XG response for a fatal error,
        and returns an appropriate exception.  Error codes extracted
        from vdmd_mgmt.c.

        Arguments:
            response: a response dict result from an XG request
        """
        # known non-fatal response codes
        #
        retry_codes = {1024: 'lun deletion in progress, try again later',
                       14032: 'lc_err_lock_busy'}

        if response['code'] == 14000:
            # lc_generic_error
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14002:
            # lc_err_assertion_failed
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14004:
            # lc_err_not_found
            raise ViolinBackendErrNotFound()
        elif response['code'] == 14005:
            # lc_err_exists
            raise ViolinBackendErrExists()
        elif response['code'] == 14008:
            # lc_err_unexpected_arg
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14014:
            # lc_err_io_error
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14016:
            # lc_err_io_closed
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14017:
            # lc_err_io_timeout
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14021:
            # lc_err_unexpected_case
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14025:
            # lc_err_no_fs_space
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14035:
            # lc_err_range
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14036:
            # lc_err_invalid_param
            raise ViolinBackendErr(message=response['message'])
        elif response['code'] == 14121:
            # lc_err_cancelled_err
            raise ViolinBackendErr(message=response['message'])
